class TransmissionResnetBlock(nn.Module):
    """
    Transmission-aware Residual Block
    在标准残差块中引入 T(x) 与 1−T(x) 的调制机制。

    Args:
        dim (int): 通道数
        padding_type (str): 填充类型
        norm_layer (nn.Module): 归一化层
        use_dropout (bool): 是否使用dropout
        use_bias (bool): 卷积是否使用bias
    """

    def __init__(self, dim, padding_type='reflect', norm_layer=nn.InstanceNorm2d,
                 use_dropout=False, use_bias=False):
        super(TransmissionResnetBlock, self).__init__()

        p = 0
        if padding_type == 'reflect':
            self.pad = nn.ReflectionPad2d(1)
        elif padding_type == 'replicate':
            self.pad = nn.ReplicationPad2d(1)
        elif padding_type == 'zero':
            p = 1
            self.pad = None
        else:
            raise NotImplementedError(f'Padding type [{padding_type}] not implemented')

        # 核心卷积模块
        conv_block = []
        if self.pad is not None:
            conv_block += [self.pad]
        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),
                       norm_layer(dim),
                       nn.ReLU(True)]

        if use_dropout:
            conv_block += [nn.Dropout(0.5)]

        if self.pad is not None:
            conv_block += [self.pad]
        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),
                       norm_layer(dim)]

        self.conv_block = nn.Sequential(*conv_block)

        # 调制系数
        self.alpha = nn.Parameter(torch.tensor(0.1))  # 控制 T(x) 权重
        self.beta = nn.Parameter(torch.tensor(0.1))   # 控制 (1 - T(x)) 权重

    def forward(self, x, T_feature=None, invT_feature=None):
        """
        Args:
            x: 当前特征 [B, C, H, W]
            T_feature: 传输特征 T(x) [B, C, H, W]
            invT_feature: 反传输特征 (1 - T(x)) [B, C, H, W]
        Returns:
            输出特征 [B, C, H, W]
        """
        out = self.conv_block(x)

        # 如果没有传入 T(x)，直接走普通残差路径
        if T_feature is None or invT_feature is None:
            return x + out

        # 调整传输特征尺寸以匹配输入特征
        if T_feature.shape[2:] != x.shape[2:]:
            T_feature = F.interpolate(T_feature, size=x.shape[2:], mode='bilinear', align_corners=False)
        if invT_feature.shape[2:] != x.shape[2:]:
            invT_feature = F.interpolate(invT_feature, size=x.shape[2:], mode='bilinear', align_corners=False)

        # Step 1: 调制逻辑
        # T(x) → 强化高透射区域（细节清晰）
        # 1 - T(x) → 加强低透射区域（颜色修复）
        modulated = out * (1 + self.alpha * T_feature) - self.beta * invT_feature

        # Step 2: 残差叠加
        out = x + modulated
        return out
