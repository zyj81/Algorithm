class TransmissionResnetBlock(nn.Module):
    """
    传输感知残差块 (Transmission-Aware Residual Block)
    在标准 ResNet 残差结构中引入 T(x) 和 (1 - T(x)) 调制。
    """
    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias, alpha=1.0, beta=1.0):
        super(TransmissionResnetBlock, self).__init__()
        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)
        self.alpha = alpha  # T(x) 权重
        self.beta = beta    # (1 - T(x)) 权重

    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):
        conv_block = []
        p = 0
        if padding_type == 'reflect':
            conv_block += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            conv_block += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),
                       norm_layer(dim),
                       nn.ReLU(True)]
        if use_dropout:
            conv_block += [nn.Dropout(0.5)]
        # 第二层
        p = 0
        if padding_type == 'reflect':
            conv_block += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            conv_block += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),
                       norm_layer(dim)]
        return nn.Sequential(*conv_block)

    def forward(self, x, T_feature, invT_feature):
        """
        x: 输入特征 [B, C, H, W]
        T_feature: 传输特征 [B, C, H, W]
        invT_feature: 反传输特征 [B, C, H, W]
        """
        # 调整传输特征尺寸匹配残差特征
        if T_feature.shape[2:] != x.shape[2:]:
            T_feature = F.interpolate(T_feature, size=x.shape[2:], mode='bilinear', align_corners=False)
            invT_feature = F.interpolate(invT_feature, size=x.shape[2:], mode='bilinear', align_corners=False)

        out = self.conv_block(x)
        # 调制：强化高传输区域，补偿低传输区域
        modulated = out * (1 + self.alpha * T_feature) - self.beta * invT_feature
        return x + modulated

# Residual Module
blocks = []
mult = 2 ** n_down
for i in range(n_blocks_res):
    blocks.append(
        TransmissionResnetBlock(
            ngf * mult,
            padding_type=padding_type,
            norm_layer=norm_layer,
            use_dropout=use_dropout,
            use_bias=use_bias
        )
    )
self.rfrm = nn.ModuleList(blocks)

T_feature, invT_feature = self.trans_estimation(input)  # 从 transmission.py 估计

out = self.wrpm(input)
out = out * T_feature  # step1融合，可保留
out = self.dm(out)

# Step 2: Residual Block 调制
for res_block in self.rfrm:
    out = res_block(out, T_feature, invT_feature)
